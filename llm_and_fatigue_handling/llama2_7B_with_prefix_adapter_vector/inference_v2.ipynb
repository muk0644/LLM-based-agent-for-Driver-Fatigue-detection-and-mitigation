{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "from model_wrapper_with_mlp_adapter import FeaturePrefixAdapter\n",
    "from faiss_vd import runtime_add, retrieve_similar_vectors\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "import torch\n",
    "\n",
    "# === Authenticate Hugging Face ===\n",
    "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "if HUGGINGFACE_TOKEN is None:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN environment variable not set.\")\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# === Config ===\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "MODEL_DIR = \"/content/llama_prefix_final_model\"\n",
    "FEATURE_DIM = 9\n",
    "EMBEDDING_DIM = 4096\n",
    "PREFIX_TOKEN_COUNT = 5\n",
    "MAX_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, token=HUGGINGFACE_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# === Quantization Config ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    quantization_config=bnb_config,  # ✅ Keep this\n",
    "    low_cpu_mem_usage=True           # ✅ Optional for Colab\n",
    "    # ❌ DO NOT add: load_in_4bit=True\n",
    ")\n",
    "\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# === Load LoRA adapter from local folder ===\n",
    "llama_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    MODEL_DIR,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=False  # ✅ explicitly tells PEFT not to go to Hugging Face Hub\n",
    ")\n",
    "llama_model.eval()\n",
    "\n",
    "# === Load MLP Prefix Adapter ===\n",
    "adapter = FeaturePrefixAdapter(\n",
    "    input_dim=FEATURE_DIM,\n",
    "    hidden_dim=256,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    num_tokens=PREFIX_TOKEN_COUNT\n",
    ")\n",
    "adapter.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"prefix_adapter.pth\")))\n",
    "adapter = adapter.to(dtype=next(llama_model.parameters()).dtype, device=DEVICE)\n",
    "adapter.eval()\n",
    "\n",
    "# === Input Features and Fatigue Levels ===\n",
    "features = [24, 8, 0.38, 0.23, 96.0, 0.4, 0.21, 8.0, 1.3]\n",
    "fatigue_levels = [\"low\", \"medium\", \"medium\"]\n",
    "\n",
    "# === Get prefix embedding\n",
    "feature_tensor = torch.tensor([features], dtype=torch.float32).to(DEVICE)\n",
    "feature_tensor = feature_tensor.to(dtype=next(llama_model.parameters()).dtype)\n",
    "prefix_embeddings = adapter(feature_tensor)\n",
    "token_matrix = prefix_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "# === Retrieve top-k similar interventions\n",
    "results = retrieve_similar_vectors(token_matrix, k=3)\n",
    "retrieved_interventions = [\n",
    "    meta.get(\"intervention\") for _, meta, _ in results\n",
    "    if meta.get(\"intervention\") and meta.get(\"intervention\").strip().lower() not in {\"\", \"none\", \"driver alert\"}\n",
    "]\n",
    "\n",
    "# === Build Prompt ===\n",
    "context = (\n",
    "    \"Previously suggested interventions for similar scenarios: \"\n",
    "    + \"; \".join(retrieved_interventions)\n",
    "    + \". \"\n",
    ") if retrieved_interventions else \"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{context}\n",
    "You are an intelligent in-cabin assistant.\n",
    "\n",
    "Fatigue levels:\n",
    "- Camera: {fatigue_levels[0]}\n",
    "- Steering: {fatigue_levels[1]}\n",
    "- Lane: {fatigue_levels[2]}\n",
    "\n",
    "Based on the above driver state and past examples, suggest an intervention to keep the driver alert.\n",
    "\n",
    "⚠️ IMPORTANT: You must output in this fixed format — no extra text.\n",
    "\n",
    "Fan: Level X      ← X is a number like 1, 2, or 3\n",
    "Music: On/Off\n",
    "Vibration: On/Off\n",
    "Reason: <short explanation of the logic>\n",
    "\n",
    "Example output:\n",
    "Fan: Level 2\n",
    "Music: On\n",
    "Vibration: Off\n",
    "Reason: High blink rate and PERCLOS indicate moderate drowsiness.\n",
    "\n",
    "Now, provide your intervention:\n",
    "\"\"\".strip()\n",
    "\n",
    "# === Tokenize prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LENGTH - PREFIX_TOKEN_COUNT)\n",
    "input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "# === Embeddings\n",
    "input_embeddings = llama_model.base_model.get_input_embeddings()(input_ids).to(dtype=prefix_embeddings.dtype)\n",
    "combined_embeddings = torch.cat([prefix_embeddings, input_embeddings], dim=1)\n",
    "\n",
    "# === Attention Mask\n",
    "prefix_attention_mask = torch.ones(1, PREFIX_TOKEN_COUNT, dtype=torch.long).to(DEVICE)\n",
    "extended_attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "\n",
    "# === Generate\n",
    "with torch.no_grad():\n",
    "    output = llama_model.generate(\n",
    "        inputs_embeds=combined_embeddings,\n",
    "        attention_mask=extended_attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# === Decode\n",
    "response = tokenizer.decode(output[0, PREFIX_TOKEN_COUNT:], skip_special_tokens=True)\n",
    "print(\"\\n=== Generated Intervention ===\")\n",
    "print(response)\n",
    "\n",
    "# === Save final vector + output\n",
    "runtime_add(token_matrix, intervention=response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
